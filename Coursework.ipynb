{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "fd4fca06-f3df-4d3d-8465-c493f7879612",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using {device}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "85307fef-752a-4a8b-86d4-fb6f5d2f3e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "22cd635c-7e1e-453a-8324-c2cf1c1812da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't need to understand this function for now.\n",
    "def load_data_CIFAR10(batch_size, resize=None):\n",
    "    \"\"\"Download the CIFAR10 dataset and then load it into memory.\"\"\"\n",
    "    trans = [torchvision.transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, torchvision.transforms.Resize(resize))\n",
    "    trans = torchvision.transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.CIFAR10(\n",
    "        root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.CIFAR10(\n",
    "        root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                            num_workers=2),\n",
    "            torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                            num_workers=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "f86b1a98-2bd4-4d28-8aa9-b4e8536adb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64 # Defines the batch size\n",
    "train_iter, test_iter = load_data_CIFAR10(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "0ecba6e7-eb01-4ee4-93c4-9ec60f808e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "X, y = next(iter(train_iter)) # Requests the first training batch\n",
    "print(X.size()) # 256 images per batch. Each image is represented by a 1 x 28 x 28 tensor (number of channels x height x width). The images are grayscale, so there is a single channel.\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f4afa-cb66-47cc-83ea-31a3467ce3b8",
   "metadata": {},
   "source": [
    "## Intermediate Block Generator\n",
    "\n",
    "for creating each intermediate block a class has been definied. This class get the number of convolutional layers, and its layers and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "67ff0d21-e530-4f8e-994a-bde7be049a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockGen(nn.Module):\n",
    "    # Creating Intermediate Blocks\n",
    "    # the length of param indicates the number of intermediate blocks\n",
    "    # each parameter is then inside the param for convolutional layers\n",
    "    def __init__(self, in_channels, out_channels, c, layers, paddings, strides, kernels):\n",
    "        super(BlockGen, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        master_model = nn.ModuleList([])\n",
    "        self.c = c\n",
    "        self.layers = layers\n",
    "        for i in range(c):\n",
    "            convs = nn.ModuleList()\n",
    "            for j in range(layers[i]):\n",
    "                convs.append(nn.Conv2d(in_channels[i][j], out_channels[i][j], kernel_size=kernels[i][j],stride= strides[i][j], padding= paddings[i][j]))\n",
    "            master_model.append(convs)\n",
    "        self.model = master_model\n",
    "        # the fully-connected layer turning m to a to calculate the weights of convolutional layer in the final equation the output should be the same value as the number of concolutional layers\n",
    "        self.fc = nn.Linear(in_channels[0][0], c)\n",
    "        self.batchnorm2d = nn.BatchNorm2d(out_channels[i][j])\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        m = []\n",
    "        for i in range(x.size()[1]):\n",
    "            m.append((float(x[i].flatten().mean())))\n",
    "        m = torch.Tensor(m).to(device)\n",
    "        a = self.fc(m)\n",
    "\n",
    "        x = self.relu(x)\n",
    "        model = self.model\n",
    "        out_list = []\n",
    "        for i, mod in enumerate(model):\n",
    "            out_list.append(0)\n",
    "            # mod = model[i]\n",
    "            out = x\n",
    "            j=0\n",
    "            for model in mod:\n",
    "                out = model(out)\n",
    "            \n",
    "            out = self.maxpool(out)\n",
    "            out_list[i] = out*a[i]\n",
    "        \n",
    "        out_list = torch.tensor(sum(out_list))    \n",
    "        return self.dropout(self.relu(self.batchnorm2d(out_list)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ed366-1730-4f9f-a495-32f60fbb0d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalModel(nn.Module):\n",
    "    # Creating Intermediate Blocks\n",
    "    # the length of param indicates the number of intermediate blocks\n",
    "    # each parameter is then inside the param for convolutional layers\n",
    "    def __init__(self, num_block, c, layers, in_channels, out_channels, kernels, paddings, strides):\n",
    "        super(FinalModel, self).__init__()\n",
    "        blocks = nn.ModuleList([])\n",
    "        for i in range(num_block):\n",
    "            blocks.append(BlockGen(c=c[i], layers=layers[i], in_channels=in_channels[i], out_channels=out_channels[i], kernels=kernels[i], paddings=paddings[i], strides=strides[i]))\n",
    "            # print('block done')\n",
    "        self.blocks = blocks\n",
    "        self.fc = nn.Linear(out_channels[-1][-1][-1], 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # creating the convolutional neurons in the layer of the block\n",
    "        out_list = nn.ModuleList([])\n",
    "        for model in self.blocks:\n",
    "            x = model(x)\n",
    "            \n",
    "        channel_avg = torch.mean(torch.flatten(x, 2), dim=2)\n",
    "        out = self.fc(channel_avg)\n",
    " \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22134dca-2b56-4a68-9171-7d4a1e30bd03",
   "metadata": {},
   "source": [
    "Defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "3ff33962-1308-4110-b98b-43bf60878756",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks = 2\n",
    "c = [2, 2]\n",
    "layers = [[3, 2], [2, 2]]\n",
    "kernels = [[[6,5,3],[4,4]],[[4,6],[3,5]]]\n",
    "paddings = [[[2,0,0],[0,0]],[[2,0],[1,0]]]\n",
    "strides = [[[1,2,1],[2,1]],[[1,1],[1,1]]]\n",
    "in_channels = [[[3,6,12],[3,6]],[[12,18],[12,24]]]\n",
    "out_channels = [[[6,12,12],[6,12]],[[18,24],[24,24]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "8cb6ae26-f43d-424b-915b-f421766d7619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinalModel(\n",
       "  (blocks): ModuleList(\n",
       "    (0): BlockGen(\n",
       "      (relu): ReLU()\n",
       "      (model): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): Conv2d(3, 6, kernel_size=(6, 6), stride=(1, 1), padding=(2, 2))\n",
       "          (1): Conv2d(6, 12, kernel_size=(5, 5), stride=(2, 2))\n",
       "          (2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1))\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): Conv2d(3, 6, kernel_size=(4, 4), stride=(2, 2))\n",
       "          (1): Conv2d(6, 12, kernel_size=(4, 4), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=3, out_features=2, bias=True)\n",
       "    )\n",
       "    (1): BlockGen(\n",
       "      (relu): ReLU()\n",
       "      (model): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): Conv2d(12, 18, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
       "          (1): Conv2d(18, 24, kernel_size=(6, 6), stride=(1, 1))\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=12, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=24, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applies Xavier initialization if the `torch.nn.Module` is `torch.nn.Linear` or `torch.nn.Conv2d`\n",
    "def init_weights(m):\n",
    "    if type(m) == torch.nn.Linear or type(m) == torch.nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "# num_outputs = 10\n",
    "model = FinalModel(num_block=num_blocks, c = c, layers = layers, in_channels=in_channels, out_channels=out_channels, kernels= kernels, paddings = paddings, strides = strides).to(device)\n",
    "model.apply(init_weights) # Applies `init_weights` to every `torch.nn.Module` inside `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "62987da8-567b-4690-8e9c-2d0a05481ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "6a8fb2b4-1828-4960-81a1-49c84469adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.9\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "27f79f66-bf58-4a62-912a-b4b0931a9a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(logits, y):\n",
    "    y_hat = logits.argmax(axis=1) # Finds the column with the highest value for each row of `logits`.\n",
    "    return (y_hat == y).float().sum() # Computes the number of times that `y_hat` and `y` match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "440558aa-4038-48a2-9668-ea8b5a882e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metric(model, data_iter, metric):\n",
    "    \"\"\"Compute the average `metric` of the model on a dataset.\"\"\"\n",
    "    c = torch.tensor(0.).to(device)\n",
    "    n = torch.tensor(0.).to(device)\n",
    "    j = 0\n",
    "    for X, y in data_iter:\n",
    "        X, y = X.to(device), y.to(device) # Moves data to `device`\n",
    "        logits = model(X)\n",
    "        c += metric(logits, y)\n",
    "        n += len(y)\n",
    "    return c / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "6698f82c-1c72-4e07-9305-e07e0c612058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.09997999668121338. Testing accuracy: 0.0997999981045723.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(f'Training accuracy: {evaluate_metric(model, train_iter, correct)}. Testing accuracy: {evaluate_metric(model, test_iter, correct)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b1a07-3d66-4feb-a942-24ff75bf12b2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "6d1d9280-c122-4640-9b38-49e889393cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[518], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_iter:\n\u001b[1;32m     13\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# Moves data to `device`\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Computes the logits for the batch of images `X`\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     l \u001b[38;5;241m=\u001b[39m loss(logits, y) \u001b[38;5;66;03m# Computes the loss given the `logits` and the class vector `y`\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Zeroes the gradients stored in the model parameters\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[501], line 18\u001b[0m, in \u001b[0;36mFinalModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m out_list \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# m = []    \u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# for i in range(x.size()[1]):\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#     m.append((float(x[i].flatten().mean())))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# m = torch.Tensor(m).to(device)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m channel_avg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m2\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[500], line 24\u001b[0m, in \u001b[0;36mBlockGen.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m     23\u001b[0m     m\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28mfloat\u001b[39m(x[i]\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mmean())))\n\u001b[0;32m---> 24\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# creating the convolutional neurons in the layer of the block\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = [] # Stores the loss for each training batch\n",
    "train_accs = [] # Stores the training accuracy after each epoch\n",
    "test_accs = [] # Stores the testing accuracy after each epoch\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch + 1}/{num_epochs}.')\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    model.train() # This is necessary because batch normalization behaves differently between training and evaluation\n",
    "\n",
    "    for X, y in train_iter:\n",
    "        X, y = X.to(device), y.to(device) # Moves data to `device`\n",
    "        logits = model(X) # Computes the logits for the batch of images `X`\n",
    "        l = loss(logits, y) # Computes the loss given the `logits` and the class vector `y`\n",
    "        optimizer.zero_grad() # Zeroes the gradients stored in the model parameters\n",
    "        l.backward() # Computes the gradient of the loss `l` with respect to the model parameters\n",
    "\n",
    "        optimizer.step() # Updates the model parameters based on the gradients stored inside them\n",
    "\n",
    "        losses.append(float(l)) # Stores the loss for this batch\n",
    "\n",
    "    with torch.no_grad(): # Computing performance metrics does not require gradients\n",
    "        model.eval() # This is necessary because batch normalization behaves differently between training and evaluation\n",
    "        train_accs.append(evaluate_metric(model, train_iter, correct))\n",
    "        test_accs.append(evaluate_metric(model, test_iter, correct))\n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "\n",
    "        print(f'Training accuracy: {train_accs[-1]}. Testing accuracy: {test_accs[-1]}. Duration: {end_time - start_time:.3f}s.') # Computes and displays training/testing dataset accuracy.\n",
    "\n",
    "plt.plot(losses) # Plots the loss for each training batch\n",
    "plt.xlabel('Training batch')\n",
    "plt.ylabel('Cross entropy loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_accs, label='Training accuracy')\n",
    "plt.plot(test_accs, label='Testing accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3664e4da-10cb-46d6-8105-46b175cad34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = next(iter(train_iter))\n",
    "len(m)\n",
    "print(f'Training accuracy: {evaluate_metric(model, m, correct)}.')\n",
    "# print(f'Training accuracy: {evaluate_metric(model, m, correct)}. Testing accuracy: {evaluate_metric(model, test_iter, correct)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "661747cc-ed17-4150-b6e0-87c41f079c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10])"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "1fc826c4-74c3-4849-9fb5-f189b873934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "ea9a0c58-b782-453a-8756-fa4730b23cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0517, -0.2033,  0.0251,  0.1640, -0.0859, -0.1489, -0.1994,  0.1988,\n",
       "        -0.1764,  0.1416], device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a35754e-d61e-4a6c-bb7b-99ad7ef1c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "1dd9a0b6-9e30-4959-b22f-a5bef3ffcb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 32, 32])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4cb19970-749c-4a08-9f70-b0f33f5d289f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "d0dcde87-61f0-46ec-ad24-523c3ad6cff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch (got input: [10], target: [64])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[462], line 160\u001b[0m\n\u001b[1;32m    157\u001b[0m     train_model(model, train_loader, test_loader , learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Evaluate on test set (optional)\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Save the trained model (optional)\u001b[39;00m\n\u001b[1;32m    163\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcifar10_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[462], line 141\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, loss_fn, test_loader, device)\u001b[0m\n\u001b[1;32m    138\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Get predictions and update accuracy metrics\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch (got input: [10], target: [64])"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# Define the Intermediate Block\n",
    "class IntermediateBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_layers):\n",
    "        super(IntermediateBlock, self).__init__()\n",
    "        self.conv_layers = nn.ModuleList([nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(in_channels * out_channels, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        channel_avg = torch.mean(x, dim=1, keepdim=True)  # Average over channels\n",
    "        weights = self.fc(channel_avg).view(-1, num_layers, 1, 1)  # Reshape weights\n",
    "        outputs = [layer(x) for layer in self.conv_layers]\n",
    "        return torch.sum(weights * torch.cat(outputs, dim=1), dim=1)\n",
    "\n",
    "# Define the Output Block\n",
    "class OutputBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(OutputBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, 64)  # Optional fully-connected layer\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        channel_avg = torch.mean(x, dim=1, keepdim=True)\n",
    "        x = self.fc1(channel_avg) if self.fc1 else x\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Define the Overall Network Architecture\n",
    "class CIFAR10Classifier(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=10, num_blocks=2, block_channels=32, num_layers_per_block=2):\n",
    "        super(CIFAR10Classifier, self).__init__()\n",
    "        self.blocks = nn.ModuleList([IntermediateBlock(in_channels, block_channels, num_layers_per_block) for _ in range(num_blocks)])\n",
    "        self.output_block = OutputBlock(block_channels * num_layers_per_block, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.output_block(x)\n",
    "\n",
    "# Helper functions for data loading and training\n",
    "def load_data(batch_size=64, use_gpu=False):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.2154, 0.2024))  # Normalize for CIFAR-10\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.CIFAR10('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        train_loader.dataset.tensors = train_loader.dataset.tensors.cuda()\n",
    "        test_loader.dataset.tensors = test_loader.dataset.tensors.cuda()\n",
    "\n",
    "    return train_loader, test_loader\n",
    "def train(model, optimizer, loss_fn, train_loader, device):\n",
    "  \"\"\"\n",
    "  Function to train the model on a single epoch.\n",
    "\n",
    "  Args:\n",
    "      model: The neural network model to be trained.\n",
    "      optimizer: The optimizer used for updating model parameters.\n",
    "      loss_fn: The loss function used for calculating loss.\n",
    "      train_loader: The data loader for the training set.\n",
    "      device: The device (CPU or GPU) to use for training.\n",
    "  \"\"\"\n",
    "  model.train()  # Set the model to training mode (affects dropout layers etc.)\n",
    "  \n",
    "  running_loss = 0.0\n",
    "  for images, labels in train_loader:\n",
    "    # Move data to the device\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    # Clear gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(images)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = loss_fn(logits, labels)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update model parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update running loss\n",
    "    running_loss += loss.item()\n",
    "  \n",
    "  # Calculate average epoch loss\n",
    "  epoch_loss = running_loss / len(train_loader)\n",
    "  \n",
    "  # Print or log training statistics (optional)\n",
    "  print(f\"Epoch {epoch + 1} - Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, learning_rate=0.001, num_epochs=10, use_gpu=False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "def test(model, loss_fn, test_loader, device):\n",
    "  \"\"\"\n",
    "  Function to evaluate the model on the testing set.\n",
    "\n",
    "  Args:\n",
    "      model: The trained neural network model.\n",
    "      loss_fn: The loss function used for calculating loss.\n",
    "      test_loader: The data loader for the testing set.\n",
    "      device: The device (CPU or GPU) to use for evaluation.\n",
    "  \"\"\"\n",
    "  model.eval()  # Set the model to evaluation mode (affects dropout layers etc.)\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "      # Move data to the device\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "      # Forward pass\n",
    "      logits = model(images)\n",
    "\n",
    "      # Calculate loss\n",
    "      loss = loss_fn(logits, labels)\n",
    "      running_loss += loss.item()\n",
    "\n",
    "      # Get predictions and update accuracy metrics\n",
    "      _, predicted = torch.max(logits.data, 1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "\n",
    "  # Calculate average test loss and accuracy\n",
    "  test_loss = running_loss / len(test_loader)\n",
    "  accuracy = correct / total\n",
    "\n",
    "  # Print or log evaluation statistics (optional)\n",
    "  print(f\"Test Loss: {test_loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Train on batches\n",
    "    train_model(model, train_loader, test_loader , learning_rate=0.01)\n",
    "\n",
    "    # Evaluate on test set (optional)\n",
    "    test(model, loss_fn, test_loader, device)\n",
    "\n",
    "# Save the trained model (optional)\n",
    "torch.save(model.state_dict(), \"cifar10_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "79334653-10b6-4233-a48d-e3ff87423328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n"
     ]
    }
   ],
   "source": [
    "model = CIFAR10Classifier().to(device) # Creates and moves the model to `device`\n",
    "model.apply(init_weights)\n",
    "train_loader, test_loader = load_data()\n",
    "train_model(model, train_loader, test_loader, use_gpu = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff16f4-efe8-4bd3-b81f-8dfff02ac865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
